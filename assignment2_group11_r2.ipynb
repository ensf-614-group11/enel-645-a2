{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNY66pKKQb1e"
      },
      "source": [
        "# ENEL 645 Assignment 2\n",
        "Group 11 Team Members: Steven Au, Laurel Flanagan, Rhys Wickens, Austen Zhang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLOOi0GQM5B"
      },
      "source": [
        "## Image Classification Transfer Learning\n",
        "Pre-trained Model: Efficient Net V2 S (https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_v2_s.html#torchvision.models.efficientnet_v2_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vV8_lUSaP4Di"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import wandb\n",
        "\n",
        "# ========================================= GLOBAL CONFIGURATION ================================================\n",
        "# Data Directories\n",
        "DATA_DIR = r\"C:\\NN Data\\garbage_data\"\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"CVPR_2024_dataset_Train\")\n",
        "VAL_DIR = os.path.join(DATA_DIR, \"CVPR_2024_dataset_Val\")\n",
        "TEST_DIR = os.path.join(DATA_DIR, \"CVPR_2024_dataset_Test\")\n",
        "\n",
        "# Model and Training Configuration\n",
        "MODEL_NAME = \"efficientnetv2s_distilbert_multimodal\"\n",
        "IMAGE_SIZE = (384, 384)\n",
        "NUM_WORKERS = 4\n",
        "NUM_CLASSES = 4\n",
        "BATCH_SIZE = 128\n",
        "MAX_LEN = 24\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE_EFFICIENTNET = 0.001\n",
        "LEARNING_RATE_DISTILBERT = 0.0001\n",
        "DROPOUT = 0.3\n",
        "CONVERGENCE_THRESHOLD = 0.001  # Minimum improvement in validation loss\n",
        "PATIENCE = 10 # Number of epochs to wait for improvement\n",
        "\n",
        "# Wandb Configuration\n",
        "WANDB_CONFIG = {\n",
        "    \"entity\": \"shcau-university-of-calgary-in-alberta\",\n",
        "    \"project\": \"transfer_learning_garbage\",\n",
        "    \"name\": \"Multimodal_Model_Train_Model\",\n",
        "    \"tags\": [\"distilBERT\", \"efficientnet\", \"CVPR_2024_dataset\"],\n",
        "    \"notes\": \"Assignment 2 Train Model\",\n",
        "    \"config\": {\"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE, \"dataset\": \"CVPR_2024_dataset\"},\n",
        "    \"job_type\": \"train\",\n",
        "    \"resume\": \"allow\",\n",
        "}\n",
        "\n",
        "# Normalization Stats\n",
        "NORMALIZATION_STATS = {\n",
        "    \"mean\": [0.485, 0.456, 0.406],\n",
        "    \"std\": [0.229, 0.224, 0.225],\n",
        "}\n",
        "\n",
        "# ========================================= HELPER FUNCTIONS ================================================\n",
        "# Initialize wandb\n",
        "def initialize_wandb():\n",
        "    if wandb.run is None:\n",
        "        wandb.init(**WANDB_CONFIG)\n",
        "\n",
        "# Extract text from file names as well as labels\n",
        "def read_text_files_with_labels(path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    class_folders = sorted(os.listdir(path))\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = os.listdir(class_path)\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "                    text = file_name_no_ext.replace('_', ' ')\n",
        "                    text_without_digits = re.sub(r'\\d+', '', text)\n",
        "                    texts.append(text_without_digits)\n",
        "                    labels.append(label_map[class_name])\n",
        "\n",
        "    return np.array(texts), np.array(labels)\n",
        "\n",
        "# ========================================= DATASET CLASSES ================================================\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, image_dataset, text_dataset):\n",
        "        self.image_dataset = image_dataset\n",
        "        self.text_dataset = text_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.image_dataset), len(self.text_dataset))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.image_dataset[idx]\n",
        "        text_data = self.text_dataset[idx]\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"input_ids\": text_data[\"input_ids\"],\n",
        "            \"attention_mask\": text_data[\"attention_mask\"],\n",
        "            \"label\": label\n",
        "        }\n",
        "\n",
        "# ========================================= MODEL DEFINITION ================================================\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "\n",
        "        # EfficientNet (Image)\n",
        "        self.image_model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Freeze feature layers\n",
        "        for param in self.image_model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        num_ftrs = self.image_model.classifier[1].in_features\n",
        "\n",
        "        # Remove EfficientNet classifier\n",
        "        self.image_model.classifier = nn.Identity()\n",
        "\n",
        "        # Project features to 256 nodes\n",
        "        self.image_fc = nn.Linear(num_ftrs, 256)\n",
        "\n",
        "        # DistilBERT (Text)\n",
        "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.text_fc = nn.Linear(self.text_model.config.hidden_size, 256)\n",
        "\n",
        "        # Normalization layers\n",
        "        self.text_norms = nn.LayerNorm(256)\n",
        "        self.image_norm = nn.LayerNorm(256)\n",
        "\n",
        "        # Feature fusion Layer (Concatenation)\n",
        "        self.fusion_fc = nn.Linear(512, self.text_model.config.hidden_size)\n",
        "\n",
        "        # Classification Layer\n",
        "        self.classifier = nn.Linear(self.text_model.config.hidden_size, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, image_inputs):\n",
        "        # Extract text features\n",
        "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_norms(self.text_fc(text_output.last_hidden_state[:, 0, :]))  # Use CLS token\n",
        "\n",
        "        # Extract image features\n",
        "        image_features = self.image_norm(self.image_fc(self.image_model(image_inputs)))\n",
        "\n",
        "        # Concatenate text and image features\n",
        "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
        "\n",
        "        # Pass through fusion and classification layers\n",
        "        combined_features = self.fusion_fc(combined_features)\n",
        "        output = self.classifier(self.dropout(combined_features))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# ========================================= EVALUATION FUNCTION ================================================\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images, input_ids, attention_mask, labels = batch[\"image\"].to(device), batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "\n",
        "\n",
        "# ========================================= TRAINING LOOP ================================================\n",
        "def train_model(model, dataloaders, criterion, optimizer, device):\n",
        "    initialize_wandb()\n",
        "\n",
        "    wandb.watch(model, log=\"all\")\n",
        "    best_val_loss = float(\"inf\")\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nStarting Epoch {epoch + 1}/{EPOCHS}\")\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        # Training phase\n",
        "        for batch in dataloaders[\"train_loader\"]:\n",
        "            # Load Best Model for Testing\n",
        "            images, input_ids, attention_mask, labels = batch[\"image\"].to(device), batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        val_loss, val_acc = evaluate_model(model, dataloaders[\"val_loader\"], device)\n",
        "        \n",
        "        # Log metrics to wandb\n",
        "        wandb.log({\"epoch\": epoch+1, \"train_loss\": total_train_loss, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {total_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_val_loss - CONVERGENCE_THRESHOLD:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "            torch.save(model.state_dict(), \"best_multimodal_model.pth\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        # Early stopping if no improvement for PATIENCE epochs\n",
        "        if epochs_without_improvement >= PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch + 1} as validation loss did not improve for {PATIENCE} epochs.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform = {\n",
        "    \"train\": transforms.Compose([\n",
        "        models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "    ]),\n",
        "    \"val\": models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(),\n",
        "    \"test\": models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(),\n",
        "}\n",
        "\n",
        "# Load datasets\n",
        "image_datasets = {\n",
        "    \"train\": datasets.ImageFolder(TRAIN_DIR, transform=transform[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(VAL_DIR, transform=transform[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(TEST_DIR, transform=transform[\"test\"]),\n",
        "}\n",
        "\n",
        "# Prepare text data\n",
        "text_train, labels_train = read_text_files_with_labels(TRAIN_DIR)\n",
        "text_val, labels_val = read_text_files_with_labels(VAL_DIR)\n",
        "text_test, labels_test = read_text_files_with_labels(TEST_DIR)\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Create data loaders\n",
        "dataloaders = {\n",
        "    \"train_loader\" : DataLoader(MultimodalDataset(image_datasets[\"train\"], CustomTextDataset(text_train, labels_train, tokenizer, MAX_LEN)), \n",
        "                            batch_size=BATCH_SIZE, shuffle=True),\n",
        "    \"val_loader\" : DataLoader(MultimodalDataset(image_datasets[\"val\"], CustomTextDataset(text_val, labels_val, tokenizer, MAX_LEN)), \n",
        "                        batch_size=BATCH_SIZE, shuffle=False),\n",
        "    \"test_loader\" : DataLoader(MultimodalDataset(image_datasets[\"test\"], CustomTextDataset(text_test, labels_test, tokenizer, MAX_LEN)), \n",
        "                        batch_size=BATCH_SIZE, shuffle=False)}\n",
        "\n",
        "# Model Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.text_model.parameters(), 'lr': LEARNING_RATE_DISTILBERT},  \n",
        "    {'params': model.image_fc.parameters(), 'lr': LEARNING_RATE_EFFICIENTNET},  \n",
        "    {'params': model.classifier.parameters(), 'lr': LEARNING_RATE_EFFICIENTNET}\n",
        "])\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Currently logged in as: shcau (shcau-university-of-calgary-in-alberta) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arkzs\\AppData\\GitHub Projects\\enel-645-a2\\wandb\\run-20250227_010230-6gj5nhoc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/6gj5nhoc' target=\"_blank\">Multimodal_Model_Train_Model</a></strong> to <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/6gj5nhoc' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/6gj5nhoc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Epoch 1/50\n",
            "Epoch 1/50, Train Loss: 37.0911, Val Loss: 0.2359, Val Acc: 0.9261\n",
            "\n",
            "Starting Epoch 2/50\n",
            "Epoch 2/50, Train Loss: 15.9234, Val Loss: 0.2263, Val Acc: 0.9178\n",
            "\n",
            "Starting Epoch 3/50\n",
            "Epoch 3/50, Train Loss: 11.3538, Val Loss: 0.2225, Val Acc: 0.9217\n",
            "\n",
            "Starting Epoch 4/50\n",
            "Epoch 4/50, Train Loss: 8.3538, Val Loss: 0.2617, Val Acc: 0.9256\n",
            "\n",
            "Starting Epoch 5/50\n",
            "Epoch 5/50, Train Loss: 7.4434, Val Loss: 0.2631, Val Acc: 0.9322\n",
            "\n",
            "Starting Epoch 6/50\n",
            "Epoch 6/50, Train Loss: 5.7203, Val Loss: 0.2844, Val Acc: 0.9161\n",
            "\n",
            "Starting Epoch 7/50\n",
            "Epoch 7/50, Train Loss: 5.8741, Val Loss: 0.2972, Val Acc: 0.9300\n",
            "\n",
            "Starting Epoch 8/50\n",
            "Epoch 8/50, Train Loss: 4.2696, Val Loss: 0.3014, Val Acc: 0.9189\n",
            "\n",
            "Starting Epoch 9/50\n",
            "Epoch 9/50, Train Loss: 4.6774, Val Loss: 0.2989, Val Acc: 0.9272\n",
            "\n",
            "Starting Epoch 10/50\n",
            "Epoch 10/50, Train Loss: 4.4777, Val Loss: 0.2999, Val Acc: 0.9261\n",
            "\n",
            "Starting Epoch 11/50\n",
            "Epoch 11/50, Train Loss: 3.3456, Val Loss: 0.3286, Val Acc: 0.9239\n",
            "\n",
            "Starting Epoch 12/50\n",
            "Epoch 12/50, Train Loss: 3.5896, Val Loss: 0.3119, Val Acc: 0.9233\n",
            "\n",
            "Starting Epoch 13/50\n",
            "Epoch 13/50, Train Loss: 3.7528, Val Loss: 0.3813, Val Acc: 0.9278\n",
            "Early stopping at epoch 13 as validation loss did not improve for 10 epochs.\n"
          ]
        }
      ],
      "source": [
        "train_model(model, dataloaders, criterion, optimizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_47156\\3391021187.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_multimodal_model.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.3438, Test Accuracy: 0.8811\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▅▂▃▅█▁▇▂▆▅▄▄▆</td></tr><tr><td>val_loss</td><td>▂▁▁▃▃▄▄▄▄▄▆▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>test_accuracy</td><td>0.88112</td></tr><tr><td>train_loss</td><td>3.75276</td></tr><tr><td>val_accuracy</td><td>0.92778</td></tr><tr><td>val_loss</td><td>0.38127</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Multimodal_Model_Train_Model</strong> at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/6gj5nhoc' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage/runs/6gj5nhoc</a><br> View project at: <a href='https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage' target=\"_blank\">https://wandb.ai/shcau-university-of-calgary-in-alberta/transfer_learning_garbage</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250227_010230-6gj5nhoc\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load Best Model for Testing\n",
        "model.load_state_dict(torch.load(\"best_multimodal_model.pth\"))\n",
        "test_loss, test_acc = evaluate_model(model, dataloaders[\"test_loader\"], device)\n",
        "wandb.log({\"test_accuracy\": test_acc})\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_47156\\2407972723.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_multimodal_model.pth\"))\n",
            "C:\\Users\\arkzs\\AppData\\Local\\Temp\\ipykernel_47156\\2407972723.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('multimodal_model_inference.pth')\n"
          ]
        }
      ],
      "source": [
        "def save_eval_model():\n",
        "    model.load_state_dict(torch.load(\"best_multimodal_model.pth\"))\n",
        "    model.eval()\n",
        "    torch.save(model.state_dict(), 'multimodal_model_inference.pth')\n",
        "\n",
        "def load_eval_model():\n",
        "    model = torch.load('multimodal_model_inference.pth')\n",
        "    return model\n",
        "\n",
        "save_eval_model()\n",
        "model = load_eval_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "enel645_torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
