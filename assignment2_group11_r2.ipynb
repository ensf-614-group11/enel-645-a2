{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNY66pKKQb1e"
      },
      "source": [
        "# ENEL 645 Assignment 2\n",
        "Group 11 Team Members: Steven Au, Laurel Flanagan, Rhys Wickens, Austen Zhang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLOOi0GQM5B"
      },
      "source": [
        "## Image Classification Transfer Learning\n",
        "Pre-trained Model: Efficient Net V2 S (https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_v2_s.html#torchvision.models.efficientnet_v2_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vV8_lUSaP4Di"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to C:\\Users\\rhysw/.cache\\torch\\hub\\checkpoints\\efficientnet_v2_s-dd5fe13b.pth\n",
            "100%|██████████| 82.7M/82.7M [00:06<00:00, 14.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "train Loss: 1.3184 Acc: 0.3830\n",
            "val Loss: 1.2754 Acc: 0.4545\n",
            "Epoch 2/20\n",
            "train Loss: 1.1786 Acc: 0.4731\n",
            "val Loss: 1.1917 Acc: 0.6477\n",
            "Epoch 3/20\n",
            "train Loss: 1.0860 Acc: 0.5754\n",
            "val Loss: 1.1136 Acc: 0.6477\n",
            "Epoch 4/20\n",
            "train Loss: 1.0211 Acc: 0.6603\n",
            "val Loss: 1.0411 Acc: 0.6818\n",
            "Epoch 5/20\n",
            "train Loss: 0.9580 Acc: 0.6620\n",
            "val Loss: 0.9873 Acc: 0.6932\n",
            "Epoch 6/20\n",
            "train Loss: 0.9145 Acc: 0.6898\n",
            "val Loss: 0.9621 Acc: 0.6591\n",
            "Epoch 7/20\n",
            "train Loss: 0.8503 Acc: 0.6967\n",
            "val Loss: 0.9268 Acc: 0.6705\n",
            "Epoch 8/20\n",
            "train Loss: 0.8257 Acc: 0.7227\n",
            "val Loss: 0.9133 Acc: 0.6705\n",
            "Epoch 9/20\n",
            "train Loss: 0.8101 Acc: 0.7175\n",
            "val Loss: 0.8939 Acc: 0.6932\n",
            "Epoch 10/20\n",
            "train Loss: 0.7831 Acc: 0.7383\n",
            "val Loss: 0.8749 Acc: 0.6932\n",
            "Epoch 11/20\n",
            "train Loss: 0.7583 Acc: 0.7383\n",
            "val Loss: 0.8663 Acc: 0.7045\n",
            "Epoch 12/20\n",
            "train Loss: 0.7296 Acc: 0.7435\n",
            "val Loss: 0.8557 Acc: 0.6932\n",
            "Epoch 13/20\n",
            "train Loss: 0.7075 Acc: 0.7591\n",
            "val Loss: 0.8490 Acc: 0.6932\n",
            "Epoch 14/20\n",
            "train Loss: 0.6759 Acc: 0.7626\n",
            "val Loss: 0.8472 Acc: 0.6932\n",
            "Epoch 15/20\n",
            "train Loss: 0.6819 Acc: 0.7660\n",
            "val Loss: 0.8295 Acc: 0.6932\n",
            "Epoch 16/20\n",
            "train Loss: 0.6804 Acc: 0.7487\n",
            "val Loss: 0.8251 Acc: 0.6932\n",
            "Epoch 17/20\n",
            "train Loss: 0.6626 Acc: 0.7487\n",
            "val Loss: 0.8266 Acc: 0.6932\n",
            "Epoch 18/20\n",
            "train Loss: 0.6398 Acc: 0.7660\n",
            "val Loss: 0.8190 Acc: 0.6932\n",
            "Epoch 19/20\n",
            "train Loss: 0.6440 Acc: 0.7556\n",
            "val Loss: 0.8155 Acc: 0.7045\n",
            "Epoch 20/20\n",
            "train Loss: 0.6141 Acc: 0.7938\n",
            "val Loss: 0.8177 Acc: 0.7045\n",
            "Best val Acc: 0.7045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rhysw\\AppData\\Local\\Temp\\ipykernel_27800\\3417665594.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 56.80%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import wandb\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define Data Directories\n",
        "data_dir = \"C:/Users/Auste/Documents/ENEL645_GarbageData/\"\n",
        "train_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Train\")\n",
        "val_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Val\")\n",
        "test_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Test\")\n",
        "\n",
        "# Initialize wandb\n",
        "def initialize_wandb():\n",
        "    if wandb.run is None:\n",
        "        wandb.init(\n",
        "            entity=\"shcau-university-of-calgary-in-alberta\",\n",
        "            project=\"transfer_learning_garbage\",\n",
        "            name=\"Multimodal_Model_RTX4060_R3\",\n",
        "            tags=[\"distilBERT\", \"efficientnet\", \"CVPR_2024_dataset\"],\n",
        "            notes=\"Multimodal classification model using distilBERT and efficientnet.\",\n",
        "            config={\"epochs\": 5, \"batch_size\": 128, \"dataset\": \"CVPR_2024_dataset\"},\n",
        "            job_type=\"train\",\n",
        "            resume=\"allow\",\n",
        "        )\n",
        "\n",
        "initialize_wandb()\n",
        "\n",
        "# Define transformations\n",
        "transform = {\n",
        "    \"train\": transforms.Compose([\n",
        "        models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(), # This includes the following preprocessing: The images are resized to resize_size=[384] using interpolation=InterpolationMode.BILINEAR,\n",
        "        # followed by a central crop of crop_size=[384]. Finally, the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]\n",
        "        transforms.RandomHorizontalFlip(), # additional data augmentation step added to training data set\n",
        "    ]),\n",
        "    \"val\": models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(),\n",
        "    \"test\": models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(),\n",
        "}\n",
        "\n",
        "# Load datasets\n",
        "image_datasets = {\n",
        "    \"train\": datasets.ImageFolder(train_dir, transform=transform[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(val_dir, transform=transform[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(test_dir, transform=transform[\"test\"]),\n",
        "}\n",
        "\n",
        "\n",
        "# Text Classification\n",
        "\n",
        "# Extract text from file names as well as labels\n",
        "def read_text_files_with_labels(path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    class_folders = sorted(os.listdir(path))\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = os.listdir(class_path)\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "                    text = file_name_no_ext.replace('_', ' ')\n",
        "                    text_without_digits = re.sub(r'\\d+', '', text)\n",
        "                    texts.append(text_without_digits)\n",
        "                    labels.append(label_map[class_name])\n",
        "\n",
        "    return np.array(texts), np.array(labels)\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# Prepare text data\n",
        "\n",
        "text_train,labels_train = read_text_files_with_labels(train_dir)\n",
        "text_val,labels_val = read_text_files_with_labels(val_dir)\n",
        "text_test,labels_test = read_text_files_with_labels(test_dir)\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "max_len = 24\n",
        "\n",
        "#Define number of epochs\n",
        "EPOCHS = 5\n",
        "\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, image_dataset, text_dataset):\n",
        "        self.image_dataset = image_dataset\n",
        "        self.text_dataset = text_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.image_dataset), len(self.text_dataset))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.image_dataset[idx]\n",
        "        text_data = self.text_dataset[idx]\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"input_ids\": text_data[\"input_ids\"],\n",
        "            \"label\": label\n",
        "        }\n",
        "    \n",
        "\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "\n",
        "        # EfficientNet (Image)\n",
        "        self.image_model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Freeze feature layers\n",
        "        for param in self.image_model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        num_ftrs = self.image_model.classifier[1].in_features\n",
        "\n",
        "        #Remove EfficientNet classifier\n",
        "        self.image_model.classifier = nn.Identity()\n",
        "\n",
        "        #Project features to 256 nodes\n",
        "        self.image_fc = nn.Linear(num_ftrs, 256)\n",
        "\n",
        "        # DistilBERT (Text)\n",
        "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.text_fc = nn.Linear(self.text_model.config.hidden_size, 256)\n",
        "\n",
        "        # Normalization layers\n",
        "        self.text_norms = nn.LayerNorm(256)\n",
        "        self.image_norm = nn.LayerNorm(256)\n",
        "\n",
        "        # Feature fusion Layer (Concatenation)\n",
        "        self.fusion_fc = nn.Linear(512, self.text_model.config.hidden_size)\n",
        "\n",
        "        # Classification Layer\n",
        "        self.classifier = nn.Linear(self.text_model.config.hidden_size, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, input_ids, image_inputs):\n",
        "        # Extract features\n",
        "        text_output = self.text_model(input_ids=input_ids)\n",
        "        text_features = self.text_norms(self.text_fc(text_output.last_hidden_state[:, 0, :]))  # Use CLS token\n",
        "        image_features = self.image_norm(self.image_fc(self.image_model(image_inputs)))\n",
        "\n",
        "        # Concatenate text and image features\n",
        "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
        "\n",
        "        combined_features = self.fusion_fc(combined_features)\n",
        "        output = self.classifier(self.dropout(combined_features))\n",
        "\n",
        "        return output\n",
        "    \n",
        "# Data Loaders\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(MultimodalDataset(image_datasets[\"train\"], CustomTextDataset(text_train, labels_train, tokenizer, max_len)), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(MultimodalDataset(image_datasets[\"val\"], CustomTextDataset(text_val, labels_val, tokenizer, max_len)), batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(MultimodalDataset(image_datasets[\"test\"], CustomTextDataset(text_test, labels_test, tokenizer, max_len)), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "        \n",
        "# Model Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultimodalClassifier(num_classes=4).to(device)\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.text_model.parameters(), 'lr': 0.0001},  \n",
        "    {'params': model.image_fc.parameters(), 'lr': 0.001},  \n",
        "    {'params': model.classifier.parameters(), 'lr': 0.001}\n",
        "])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# Training\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        images = batch[\"image\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_multimodal_model.pth\")\n",
        "\n",
        "    wandb.log({\"epoch\": epoch+1, \"train_loss\": total_train_loss, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
        "    print(f\"Epoch {epoch+1}/{5}, Train Loss: {total_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Load Best Model for Testing\n",
        "model.load_state_dict(torch.load(\"best_multimodal_model.pth\"))\n",
        "test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
        "wandb.log({\"test_accuracy\": test_acc})\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Classification Transfer Learning\n",
        "Pre-trained model: DistilBERT Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define functions\n",
        "\n",
        "# Extract text from file names as well as labels\n",
        "def read_text_files_with_labels(path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    class_folders = sorted(os.listdir(path))  # Assuming class folders are sorted\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = os.listdir(class_path)\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "                    text = file_name_no_ext.replace('_', ' ')\n",
        "                    text_without_digits = re.sub(r'\\d+', '', text)\n",
        "                    texts.append(text_without_digits)\n",
        "                    labels.append(label_map[class_name])\n",
        "\n",
        "    return np.array(texts), np.array(labels)\n",
        "\n",
        "# Define your dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define the model\n",
        "class DistilBERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DistilBERTClassifier, self).__init__()\n",
        "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(self.distilbert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        pooled_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "        output = self.drop(pooled_output[:,0])\n",
        "        return self.out(output)\n",
        "\n",
        "# Define training function\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in iterator:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_ids, attention_mask)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(iterator)\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval() # set model to evaluation model\n",
        "    total_loss = 0\n",
        "    with torch.no_grad(): # don't need dropout to be active so we disable gradients\n",
        "        for batch in iterator:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            output = model(input_ids, attention_mask)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(iterator)\n",
        "\n",
        "# Define prediction function\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions = []\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)  # Assuming input_ids are in the batch\n",
        "            attention_mask = batch['attention_mask'].to(device)  # Assuming attention_mask is in the batch\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            # Convert predictions to CPU and append to the list\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the paths for the data from the image classification\n",
        "\n",
        "text_train,labels_train = read_text_files_with_labels(train_dir)\n",
        "text_val,labels_val = read_text_files_with_labels(val_dir)\n",
        "text_test,labels_test = read_text_files_with_labels(test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18ee9d72b8014026a54df4e0313790b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rhysw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rhysw\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8798f57eb345446d98cd3c2e0b7404cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "601d9109c11f4b6da619ca3e56919f51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "418e5c953110417a83986177b5f230a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74927ef5e5db46e1bdf05acdcb977914",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train Loss: 1.2118\n",
            "Epoch: 1, Val Loss: 0.9150\n",
            "Epoch: 2, Train Loss: 0.7579\n",
            "Epoch: 2, Val Loss: 0.5738\n",
            "Epoch: 3, Train Loss: 0.4443\n",
            "Epoch: 3, Val Loss: 0.5192\n",
            "Epoch: 4, Train Loss: 0.2504\n",
            "Epoch: 4, Val Loss: 0.5777\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import wandb\n",
        "\n",
        "# Define Data Directories\n",
        "data_dir = \"C:/Users/Auste/Documents/ENEL645_GarbageData/\"\n",
        "train_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Train\")\n",
        "val_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Val\")\n",
        "test_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Test\")\n",
        "\n",
        "# Initialize wandb\n",
        "def initialize_wandb():\n",
        "    if wandb.run is None:\n",
        "        wandb.init(\n",
        "            entity=\"shcau-university-of-calgary-in-alberta\",\n",
        "            project=\"transfer_learning_garbage\",\n",
        "            name=\"Multimodal_Model_RTX4060_R3\",\n",
        "            tags=[\"distilBERT\", \"efficientnet\", \"CVPR_2024_dataset\"],\n",
        "            notes=\"Multimodal classification model using distilBERT and efficientnet.\",\n",
        "            config={\"epochs\": 5, \"batch_size\": 128, \"dataset\": \"CVPR_2024_dataset\"},\n",
        "            job_type=\"train\",\n",
        "            resume=\"allow\",\n",
        "        )\n",
        "\n",
        "initialize_wandb()\n",
        "\n",
        "# Define transformations\n",
        "transform = {\n",
        "    \"train\": transforms.Compose([\n",
        "        models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(), # This includes the following preprocessing: The images are resized to resize_size=[384] using interpolation=InterpolationMode.BILINEAR,\n",
        "        # followed by a central crop of crop_size=[384]. Finally, the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]\n",
        "        transforms.RandomHorizontalFlip(), # additional data augmentation step added to training data set\n",
        "    ]),\n",
        "    \"val\": models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(),\n",
        "    \"test\": models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(),\n",
        "}\n",
        "\n",
        "# Load datasets\n",
        "image_datasets = {\n",
        "    \"train\": datasets.ImageFolder(train_dir, transform=transform[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(val_dir, transform=transform[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(test_dir, transform=transform[\"test\"]),\n",
        "}\n",
        "\n",
        "\n",
        "# Text Classification\n",
        "\n",
        "# Extract text from file names as well as labels\n",
        "def read_text_files_with_labels(path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    class_folders = sorted(os.listdir(path))\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = os.listdir(class_path)\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "                    text = file_name_no_ext.replace('_', ' ')\n",
        "                    text_without_digits = re.sub(r'\\d+', '', text)\n",
        "                    texts.append(text_without_digits)\n",
        "                    labels.append(label_map[class_name])\n",
        "\n",
        "    return np.array(texts), np.array(labels)\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# Prepare text data\n",
        "\n",
        "text_train,labels_train = read_text_files_with_labels(train_dir)\n",
        "text_val,labels_val = read_text_files_with_labels(val_dir)\n",
        "text_test,labels_test = read_text_files_with_labels(test_dir)\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "max_len = 24\n",
        "\n",
        "#Define number of epochs\n",
        "EPOCHS = 5\n",
        "\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, image_dataset, text_dataset):\n",
        "        self.image_dataset = image_dataset\n",
        "        self.text_dataset = text_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.image_dataset), len(self.text_dataset))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.image_dataset[idx]\n",
        "        text_data = self.text_dataset[idx]\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"input_ids\": text_data[\"input_ids\"],\n",
        "            \"label\": label\n",
        "        }\n",
        "    \n",
        "\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "\n",
        "        # EfficientNet (Image)\n",
        "        self.image_model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Freeze feature layers\n",
        "        for param in self.image_model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        num_ftrs = self.image_model.classifier[1].in_features\n",
        "\n",
        "        #Remove EfficientNet classifier\n",
        "        self.image_model.classifier = nn.Identity()\n",
        "\n",
        "        #Project features to 256 nodes\n",
        "        self.image_fc = nn.Linear(num_ftrs, 256)\n",
        "\n",
        "        # DistilBERT (Text)\n",
        "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.text_fc = nn.Linear(self.text_model.config.hidden_size, 256)\n",
        "\n",
        "        # Normalization layers\n",
        "        self.text_norms = nn.LayerNorm(256)\n",
        "        self.image_norm = nn.LayerNorm(256)\n",
        "\n",
        "        # Feature fusion Layer (Concatenation)\n",
        "        self.fusion_fc = nn.Linear(512, self.text_model.config.hidden_size)\n",
        "\n",
        "        # Classification Layer\n",
        "        self.classifier = nn.Linear(self.text_model.config.hidden_size, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, input_ids, image_inputs):\n",
        "        # Extract features\n",
        "        text_output = self.text_model(input_ids=input_ids)\n",
        "        text_features = self.text_norms(self.text_fc(text_output.last_hidden_state[:, 0, :]))  # Use CLS token\n",
        "        image_features = self.image_norm(self.image_fc(self.image_model(image_inputs)))\n",
        "\n",
        "        # Concatenate text and image features\n",
        "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
        "\n",
        "        combined_features = self.fusion_fc(combined_features)\n",
        "        output = self.classifier(self.dropout(combined_features))\n",
        "\n",
        "        return output\n",
        "    \n",
        "# Data Loaders\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(MultimodalDataset(image_datasets[\"train\"], CustomTextDataset(text_train, labels_train, tokenizer, max_len)), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(MultimodalDataset(image_datasets[\"val\"], CustomTextDataset(text_val, labels_val, tokenizer, max_len)), batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(MultimodalDataset(image_datasets[\"test\"], CustomTextDataset(text_test, labels_test, tokenizer, max_len)), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "        \n",
        "# Model Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultimodalClassifier(num_classes=4).to(device)\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.text_model.parameters(), 'lr': 0.0001},  \n",
        "    {'params': model.image_fc.parameters(), 'lr': 0.001},  \n",
        "    {'params': model.classifier.parameters(), 'lr': 0.001}\n",
        "])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# Training\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        images = batch[\"image\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_multimodal_model.pth\")\n",
        "\n",
        "    wandb.log({\"epoch\": epoch+1, \"train_loss\": total_train_loss, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
        "    print(f\"Epoch {epoch+1}/{5}, Train Loss: {total_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rhysw\\AppData\\Local\\Temp\\ipykernel_27800\\2595054650.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  text_model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7811\n"
          ]
        }
      ],
      "source": [
        "# Load Best Model for Testing\n",
        "model.load_state_dict(torch.load(\"best_multimodal_model.pth\"))\n",
        "test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
        "wandb.log({\"test_accuracy\": test_acc})\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
