{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNY66pKKQb1e"
      },
      "source": [
        "# ENEL 645 Assignment 2\n",
        "Group 11 Team Members: Steven Au, Laurel Flanagan, Rhys Wickens, Austen Zhang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLOOi0GQM5B"
      },
      "source": [
        "## Image Classification Transfer Learning\n",
        "Pre-trained Model: Efficient Net V2 S (https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_v2_s.html#torchvision.models.efficientnet_v2_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vV8_lUSaP4Di"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to C:\\Users\\rhysw/.cache\\torch\\hub\\checkpoints\\efficientnet_v2_s-dd5fe13b.pth\n",
            "100%|██████████| 82.7M/82.7M [00:06<00:00, 14.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "train Loss: 1.3184 Acc: 0.3830\n",
            "val Loss: 1.2754 Acc: 0.4545\n",
            "Epoch 2/20\n",
            "train Loss: 1.1786 Acc: 0.4731\n",
            "val Loss: 1.1917 Acc: 0.6477\n",
            "Epoch 3/20\n",
            "train Loss: 1.0860 Acc: 0.5754\n",
            "val Loss: 1.1136 Acc: 0.6477\n",
            "Epoch 4/20\n",
            "train Loss: 1.0211 Acc: 0.6603\n",
            "val Loss: 1.0411 Acc: 0.6818\n",
            "Epoch 5/20\n",
            "train Loss: 0.9580 Acc: 0.6620\n",
            "val Loss: 0.9873 Acc: 0.6932\n",
            "Epoch 6/20\n",
            "train Loss: 0.9145 Acc: 0.6898\n",
            "val Loss: 0.9621 Acc: 0.6591\n",
            "Epoch 7/20\n",
            "train Loss: 0.8503 Acc: 0.6967\n",
            "val Loss: 0.9268 Acc: 0.6705\n",
            "Epoch 8/20\n",
            "train Loss: 0.8257 Acc: 0.7227\n",
            "val Loss: 0.9133 Acc: 0.6705\n",
            "Epoch 9/20\n",
            "train Loss: 0.8101 Acc: 0.7175\n",
            "val Loss: 0.8939 Acc: 0.6932\n",
            "Epoch 10/20\n",
            "train Loss: 0.7831 Acc: 0.7383\n",
            "val Loss: 0.8749 Acc: 0.6932\n",
            "Epoch 11/20\n",
            "train Loss: 0.7583 Acc: 0.7383\n",
            "val Loss: 0.8663 Acc: 0.7045\n",
            "Epoch 12/20\n",
            "train Loss: 0.7296 Acc: 0.7435\n",
            "val Loss: 0.8557 Acc: 0.6932\n",
            "Epoch 13/20\n",
            "train Loss: 0.7075 Acc: 0.7591\n",
            "val Loss: 0.8490 Acc: 0.6932\n",
            "Epoch 14/20\n",
            "train Loss: 0.6759 Acc: 0.7626\n",
            "val Loss: 0.8472 Acc: 0.6932\n",
            "Epoch 15/20\n",
            "train Loss: 0.6819 Acc: 0.7660\n",
            "val Loss: 0.8295 Acc: 0.6932\n",
            "Epoch 16/20\n",
            "train Loss: 0.6804 Acc: 0.7487\n",
            "val Loss: 0.8251 Acc: 0.6932\n",
            "Epoch 17/20\n",
            "train Loss: 0.6626 Acc: 0.7487\n",
            "val Loss: 0.8266 Acc: 0.6932\n",
            "Epoch 18/20\n",
            "train Loss: 0.6398 Acc: 0.7660\n",
            "val Loss: 0.8190 Acc: 0.6932\n",
            "Epoch 19/20\n",
            "train Loss: 0.6440 Acc: 0.7556\n",
            "val Loss: 0.8155 Acc: 0.7045\n",
            "Epoch 20/20\n",
            "train Loss: 0.6141 Acc: 0.7938\n",
            "val Loss: 0.8177 Acc: 0.7045\n",
            "Best val Acc: 0.7045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rhysw\\AppData\\Local\\Temp\\ipykernel_27800\\3417665594.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 56.80%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "\n",
        "# Define data directories\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/ENEL 645/CVPR_2024_dataset_Subset\" #UPDATE DIRECTORY\n",
        "# data_dir = r\"C:\\Users\\rhysw\\OneDrive\\Documents\\UofC Masters\\Winter\\ENEL 645\\Assignments\\A2\" #UPDATE DIRECTORY\n",
        "train_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Train\")\n",
        "val_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Val\")\n",
        "test_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Test\")\n",
        "\n",
        "# Define transformations\n",
        "transform = {\n",
        "    \"train\": transforms.Compose([\n",
        "        models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(), # This includes the following preprocessing: The images are resized to resize_size=[384] using interpolation=InterpolationMode.BILINEAR,\n",
        "        # followed by a central crop of crop_size=[384]. Finally, the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]\n",
        "        transforms.RandomHorizontalFlip(), # additional data augmentation step added to training data set\n",
        "    ]),\n",
        "    \"val\": models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(),\n",
        "    \"test\": models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(),\n",
        "}\n",
        "\n",
        "# Load datasets\n",
        "datasets = {\n",
        "    \"train\": datasets.ImageFolder(train_dir, transform=transform[\"train\"]), #converts Image folder into PyTorch dataset\n",
        "    \"val\": datasets.ImageFolder(val_dir, transform=transform[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(test_dir, transform=transform[\"test\"]),\n",
        "}\n",
        "\n",
        "# Define data loaders\n",
        "dataloaders = {\n",
        "    \"train\": DataLoader(datasets[\"train\"], batch_size=128, shuffle=True, num_workers=4),\n",
        "    \"val\": DataLoader(datasets[\"val\"], batch_size=128, shuffle=False, num_workers=4),\n",
        "    \"test\": DataLoader(datasets[\"test\"], batch_size=128, shuffle=False, num_workers=4),\n",
        "\n",
        "}\n",
        "\n",
        "# Load the pre-trained EfficientNet_V2_S model with recommended weights\n",
        "model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze all layers except the last classifier\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier for 4-class classification\n",
        "num_features = model.classifier[1].in_features # input size for last fully connected layer\n",
        "model.classifier[1] = nn.Linear(num_features, 4) # output size of 4 classes\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier[1].parameters(), lr=0.001) # optimize parameters of classifier, not feature extraction layers\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10):\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == \"train\":\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval() # turns off layers not needed during prediction, like dropout layer\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "            if phase == \"val\" and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model, dataloaders, criterion, optimizer, num_epochs=20)\n",
        "\n",
        "# Test function\n",
        "def test_model(model, dataloader):\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1) # use max function to get exact label\n",
        "            correct += torch.sum(preds == labels).item()\n",
        "            total += labels.size(0)\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_model(model, dataloaders[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Classification Transfer Learning\n",
        "Pre-trained model: DistilBERT Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define functions\n",
        "\n",
        "# Extract text from file names as well as labels\n",
        "def read_text_files_with_labels(path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    class_folders = sorted(os.listdir(path))  # Assuming class folders are sorted\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = os.listdir(class_path)\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "                    text = file_name_no_ext.replace('_', ' ')\n",
        "                    text_without_digits = re.sub(r'\\d+', '', text)\n",
        "                    texts.append(text_without_digits)\n",
        "                    labels.append(label_map[class_name])\n",
        "\n",
        "    return np.array(texts), np.array(labels)\n",
        "\n",
        "# Define your dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define the model\n",
        "class DistilBERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DistilBERTClassifier, self).__init__()\n",
        "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(self.distilbert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        pooled_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "        output = self.drop(pooled_output[:,0])\n",
        "        return self.out(output)\n",
        "\n",
        "# Define training function\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in iterator:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_ids, attention_mask)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(iterator)\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval() # set model to evaluation model\n",
        "    total_loss = 0\n",
        "    with torch.no_grad(): # don't need dropout to be active so we disable gradients\n",
        "        for batch in iterator:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            output = model(input_ids, attention_mask)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(iterator)\n",
        "\n",
        "# Define prediction function\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions = []\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)  # Assuming input_ids are in the batch\n",
        "            attention_mask = batch['attention_mask'].to(device)  # Assuming attention_mask is in the batch\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            # Convert predictions to CPU and append to the list\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the paths for the data from the image classification\n",
        "\n",
        "text_train,labels_train = read_text_files_with_labels(train_dir)\n",
        "text_val,labels_val = read_text_files_with_labels(val_dir)\n",
        "text_test,labels_test = read_text_files_with_labels(test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18ee9d72b8014026a54df4e0313790b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rhysw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rhysw\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8798f57eb345446d98cd3c2e0b7404cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "601d9109c11f4b6da619ca3e56919f51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "418e5c953110417a83986177b5f230a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74927ef5e5db46e1bdf05acdcb977914",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train Loss: 1.2118\n",
            "Epoch: 1, Val Loss: 0.9150\n",
            "Epoch: 2, Train Loss: 0.7579\n",
            "Epoch: 2, Val Loss: 0.5738\n",
            "Epoch: 3, Train Loss: 0.4443\n",
            "Epoch: 3, Val Loss: 0.5192\n",
            "Epoch: 4, Train Loss: 0.2504\n",
            "Epoch: 4, Val Loss: 0.5777\n"
          ]
        }
      ],
      "source": [
        "# Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize data\n",
        "max_len = 24\n",
        "dataset_train = CustomDataset(text_train, labels_train, tokenizer, max_len)\n",
        "dataset_val = CustomDataset(text_val, labels_val, tokenizer, max_len)\n",
        "dataset_test = CustomDataset(text_test, labels_test, tokenizer, max_len)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(dataset_val, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(dataset_test, batch_size=128, shuffle=False)\n",
        "\n",
        "best_loss = 1e+10 # best loss tracker\n",
        "EPOCHS = 4\n",
        "\n",
        "# Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "text_model = DistilBERTClassifier(num_classes=4).to(device)\n",
        "\n",
        "# Training parameters\n",
        "optimizer = optim.Adam(text_model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train(text_model, train_loader, optimizer, criterion, device)\n",
        "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}')\n",
        "    val_loss = evaluate(text_model, val_loader, criterion, device)\n",
        "    print(f'Epoch: {epoch+1}, Val Loss: {val_loss:.4f}')\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(text_model.state_dict(), 'best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rhysw\\AppData\\Local\\Temp\\ipykernel_27800\\2595054650.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  text_model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7811\n"
          ]
        }
      ],
      "source": [
        "text_model.load_state_dict(torch.load('best_model.pth'))\n",
        "# Evaluation\n",
        "test_predictions = predict(text_model, test_loader, device)\n",
        "print(f\"Accuracy:  {(test_predictions == labels_test).sum()/labels_test.size:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = confusion_matrix(labels_test, test_predictions)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
